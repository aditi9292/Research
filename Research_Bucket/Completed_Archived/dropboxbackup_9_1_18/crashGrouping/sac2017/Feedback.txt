Dear Dr. Daniel Krutz:

I am sorry to inform you that the following submission 
was not selected by the program committee to appear at 
SAC 2017: 

     An Empirical Analysis of Crash Dump Grouping: How to
          Effectively Group Crash Dumps

The selection process was very competitive. Due to time 
and space limitations, we could only choose a small number 
of the submitted papers to appear on the program.  Nonetheless, 
I still hope you can attend the conference. 

I have enclosed the reviewer comments for your perusal.

If you have any additional questions, please feel free 
to get in touch.

Best Regards,
Yliès Falcone and Mercedes G. Merayo
SAC 2017, SVT - Software Verification and Testing

============================================================================ 
SAC 2017 Reviews for Submission #1009
============================================================================ 

Title: An Empirical Analysis of Crash Dump Grouping: How to Effectively Group Crash Dumps

Authors: Daniel Krutz and Samuel Malachowsky
============================================================================
                           REVIEWER #1
============================================================================ 


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------

         Technical Content and Accuracy: 4
               Significance of the Work: 4
Appropriate Title, Introduction, and Conclusion: 5
                   Overall Organization: 4
                Appropriateness for SAC: 1
         Style and Clarity of the Paper: 3
                 Originality of Content: 4
                 OVERALL RECOMMENDATION: 2


---------------------------------------------------------------------------
Comments
---------------------------------------------------------------------------

The paper presents an empirical analysis of crash dumps that has been manually
and automatically grouped. The approach has been applied to Mozilla
applications, using the Mozilla Crash Reporter.  

The approach is completely experimental and there is no use of formal methods.
This is not bad per se, but it is clearly out of scope of this track. As
clearly stated in the description of the SVT track, "[t]he Software
Verification and Testing track aims at contributing to the challenge of
improving the usability of formal methods in software engineering. The track
covers areas such as formal methods for verification and testing, based on
theorem proving, model checking, static analysis, and run-time verification."

I suggest that authors to resubmit their work to a more suitable venue.

============================================================================
                           REVIEWER #2
============================================================================ 


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------

         Technical Content and Accuracy: 6
               Significance of the Work: 5
Appropriate Title, Introduction, and Conclusion: 6
                   Overall Organization: 6
                Appropriateness for SAC: 3
         Style and Clarity of the Paper: 6
                 Originality of Content: 5
                 OVERALL RECOMMENDATION: 4


---------------------------------------------------------------------------
Comments
---------------------------------------------------------------------------

The paper describes an empirical investigation into clustering application
crash dumps to better examine their root causes. The study considers over 5000
crash dumps from five popular Mozilla projects including Thunderbird and
Firefox.

The authors compare both manual as well as automatic clustering methods. The
results are, however, somewhat sobering. Both methods
have their drawbacks and can be imprecise. Of course automatic methods have a
benefit of not costing much time/effort. Moreover, both manual and automatic
imprecision as somewhat complementary. 

The value of this paper is clearly in the amount of effort spent to investigate
and understand the Mozilla crash reporting systems, it's grouping and the
investigation of the bug tracking logs.

Moreover, the paper is overall well written and provides a lot of real word
data that should be considered. It is reasonably well written an easy to
follow. However, it does not really coincide with SVTs theme of usability 
of formal methods in software engineering and  might be considered
slightly off topic.

An open question is, whether it would make sense to being able to create better
crash dumps with more information that would aid grouping as well as rout cause
identification? How would this look like? This is something the authors might
comment on in more detail should the paper be accepted.

============================================================================
                           REVIEWER #3
============================================================================ 


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------

         Technical Content and Accuracy: 6
               Significance of the Work: 5
Appropriate Title, Introduction, and Conclusion: 5
                   Overall Organization: 7
                Appropriateness for SAC: 7
         Style and Clarity of the Paper: 7
                 Originality of Content: 4
                 OVERALL RECOMMENDATION: 5


---------------------------------------------------------------------------
Comments
---------------------------------------------------------------------------

This is an interesting empirical analysis of crash dump grouping. The paper
only analysis Mozilla, which may reduce the generability of the approach. Paper
is well written and structured.

Questions/concerns:
-- The authors mention that "MCSG measures call stack similarity using the call
stack similarity metrics as defined in these works". The one used in this paper
should be detailed in this paper to make it self contained. 

-- The following sentence looks pleading to me "We spent a large amount of time
optimizing our comparison algorithm and verifying its accuracy". What does it
actually imply? Isn't this true for any research paper/algorithm?

-- I don't understand the usage of httpunit. As far as I understand, this is an
automated testing framework. Are you re-running the software, or just
collecting reports available from MCR? If you're re-running, how do you ensure
you have the proper test suites? 

-- It is not clear to me who did create the m-groups? Are they available? Were
the authors? Did you ask the developers?

-- I do not understand the description of T_c in Table 2. Is this only for the
m-groups? What is T_c for a-groups?

-- Section 3.2 would benefit from a summary answering the research question. I
am not following the discussion in this section.

-- Section 3.3, summary's first bullet is very generic. What are the sort of
mistakes that developers do? Can we improve that using information foraging
(see the paper below) to improve it?

Perez, Alexandre, and Rui Abreu. "Framing program comprehension as fault
localization." Journal of Software: Evolution and Process (2016).

-- Fundamental question: why focusing on Mozilla only? It reduces considerably
the scope of this work.