Dear Daniel,

We apologize that the previous email did not include the reviews or meta-review.  
Please find them below.  Here are the reviews for your paper

Examining the Relationship between Security Metrics and User Ratings of Mobile Apps: A Case Study

Sincerely, 
Romain Robbes and Christian Bird
MSR 2016 Program Co-chairs


----------------------- REVIEW 1 ---------------------
PAPER: 27
TITLE: Examining the Relationship between Security Metrics and User Ratings of Mobile Apps: A Case Study
AUTHORS: Daniel Krutz, Casey Klimkowsky, Shannon Trudeau, Adam Blaine, Meiyappan Nagappan, Andrew Meneely and Sam Malachowsky


----------- REVIEW -----------
This paper explores the relationship between security metrics and user ratings, in particular the authors try to answer the question whether low ratings of apps may somehow be indicative of security risks. 

The research question asked is worthwhile to study but I found that the research method used was not appropriate for answering this (rather potentially complex) question.  The question asked, in my opinion, is an exploratory question about how security aspects (over or under permissions, and permission requests) may impact user perception about quality, and to study it using data alone that considers just this narrow aspect leaves out many other potential reasons for low app ratings.    Although the authors do consider different categories of apps, other aspects of the apps will affect rating, such as the number of features they provide, how often they crash, aesthetics, among others.     There is also the tradeoff between over and under permissions, as well as permission requests that can lead to a complex impact on the users. The possible explanations for the results that you give in fact hint that other methods should be used to answer this question, most notably intervie!
 wing, observing or surveying users.    The fact that much of the previous research you report is also contradictory, points to a need for richer methods to understand the context behind the contradictory findings.

In terms of the method you used, it didn’t make sense to me to compare high to low rated apps (and the arbitrary threshold of 3), why not just look at the relationship between user rating and the security metrics?  The relationship may be quite complex (with interactions between over/under/request permissions). 

Also in section 2.1, you mention that you downloaded 70,000 apps, but later mention that the potential population of apps was 1.9 million apps, how did you select those 70,000 apps?  (how you narrowed the set after that made sense). 

Writing issues:
-	The abstract and the introduction were very difficult to read – I think it is because they both contain too much speculation that should be deferred to the discussion of the paper (e.g., that adding new features may lead to security risks).   
-	Combine sections 2.1 and 2.2 into one section.
-	Remove section 2.4, it is unnecessary. 
-	RQ1, there is only one RQ so no need to number it.
-	The paper is quite repetitive, especially in the introduction and in section 3 (motivation section, one paragraph is basically repeated)
-	At the start of findings, you say “at an overall level”, what do you mean by that?
-	The conclusion you wrote (shown in the box in section 3) is based more on the static analysis tools you used, rather than the study you did?  You don’t say what the correlation refers to in this box.  I would just remove this. 
-	The related work section was very interesting to read, but I felt it should come earlier in the paper – use it to frame your research question. 

I see from your acknowledgement that you have submitted this before.  This research has the potential to be really interesting work!  But I suggest you consider using other methods to supplement your work to dig deeper into the impact of security aspects on user ratings.


----------------------- REVIEW 2 ---------------------
PAPER: 27
TITLE: Examining the Relationship between Security Metrics and User Ratings of Mobile Apps: A Case Study
AUTHORS: Daniel Krutz, Casey Klimkowsky, Shannon Trudeau, Adam Blaine, Meiyappan Nagappan, Andrew Meneely and Sam Malachowsky


----------- REVIEW -----------
Summary: The short research paper "Examining the Relationship between Security Metrics and User Ratings of Mobile Apps: A Case Study" seeks to find out if "bad security metrics" as analyzed by several automatic static analysis tools (ASATs) negatively affect user ratings of Android apps in the Google play store. Compared with several security metrics, the paper finds significant correlations with weak effect sizes throughout for the hypothesis that low-ranked apps have more security problems. This would suggest that 'bad security' as measured in the source code would somehow translate to more negative user ratings.

Verdict: The paper is well-written and the idea of observing security in apps and their effects on user ratings novel and worthwhile. However, by omitting to measure overall quality, I argue that the paper does not measure what it says it measures, and therefore might lead to potentially harmful, false conclusions. As such, I am against accepting this paper in its current form (-2).

Criticism:
+ Novel idea that is worth exploring
+ Paper is well-written

Neutral:
± The idea behind the paper is based on the assumption that ASATs can capture the 'security' of an app, which is arguably not the case. However, as explained in the paper, they might be a proxy for it, so might be worth exploring nonetheless.

- Major: My biggest concern about the paper is the unsoundness of the scientific method to evaluate the influence of security problems: By just measuring one aspect of the 'quality' of the app (security), it leaves out all other aspects that have potentially far greater effects. Even more so, the paper does not control for the general code quality of the app, which could be similarly measured by another set of ASATs. It stands to reason that the security aspect that is being measured is only a very small sub-set of the general software quality to users, and arguably the least observable: a crashing app might cause much lower user ratings than if WhatsApp does not encrypt its messages, which 99.9% of users would never be able to tell. This argument might reveal why there are only very weak correlations, since the measured variable ('security quality') might be partly an outcome of the 'real' responsible variable ('overall code quality'). To be able to tell whether 'bad securi!
 ty' has an impact on app ratings, this should be measured while controlling for overall code quality. My personal thinking is that you are implicitly really only measuring the latter, but by ignoring it, there might be the hint that users care about security--which they might not, simply because they cannot observe it. The discussion gives the hint that the authors might be aware of this.
Please note that I am empathetically not against reporting weak correlations, but in this case, there seems to be a very promising direction for a more sound analysis.

- Medium: A smaller methodological concern about the paper is that apps are arbitrarily grouped into 2 groups based on whether they have a 3 star rating, or more. As the distribution that this grouping of apps induces is highly skewed towards high-rated apps, I am very skeptical about it without any further rationale. Why is a grouping needed at all? Shouldn't the correlations (or for that matter, a more sophisticated regression model) work even better without the potential bias a grouping might introduce?

- Minor: Another smaller concern is that an app might just use one of the tools you proposed, and therefore score very good in this metric, while its 'real' security is still very bad. Have you tried to control for this effect?
- Minor: How did you select the 70,000 apps you mined from the whole 1.9 million apps?
- Minor: Due to the completely reworked Android 6 permission system, it is questionable if the lessons learnt from an analysis of pre-Android 6 are of anything but historic interest and how they might be useful for the future.
- Minor: Has the claim that the success of apps depends on user ratings (both in the abstract and introduction) been proven? Might it, again, not be a reflection of some underlying usefulness or quality property of the app?
- Minor: Typos: well known -> well-known (p. 1), RQ1: "Do apps with low[-/ ]rat[ed/ings] have more security risks" (p. 2)


----------------------- REVIEW 3 ---------------------
PAPER: 27
TITLE: Examining the Relationship between Security Metrics and User Ratings of Mobile Apps: A Case Study
AUTHORS: Daniel Krutz, Casey Klimkowsky, Shannon Trudeau, Adam Blaine, Meiyappan Nagappan, Andrew Meneely and Sam Malachowsky


----------- REVIEW -----------
This paper investigates whether security metrics, extracted by using tools such as Stowaway and Androrisk, can correlate with the user rating of Android apps. The authors have compared the user rating of a total of 2050 user apps with a rating of 3 or greater against 2050 apps with a score less than 3, and found that only the number of requested permissions positively correlates with the security metrics. Metrics such as under permission, over permission and Androrisk score only negatively correlate.

The positive aspects of this paper are the very timely and important topic and the large study. Also, IMHO the intuition that very likely permission-related metrics may correlate with the end-user satisfaction make sense. The statistical treatment is also sound and appropriate (although probably the comparison of security metrics between highly-rated and lowly-rated apps would have benefited of an effect size computation too). Last, but not least, the paper is generally well written.

At the same time, the paper has a number of weaknesses:
- This phenomenon (under-over permission) has been largely investigated by the research community, even though without correlating it with user reviews. However, relevant papers in the area are not mentioned. I understand this is a short paper with a limited space, however these are very relevant work:
H. Peng, C. S. Gates, B. P. Sarma, N. Li, Y. Qi, R. Potharaju, C. Nita-Rotaru, and I. Molloy, “Using probabilistic generative models for ranking risks of Android apps,” in the ACM Conference on Computer and Communications Security, CCS’12, Raleigh, NC, USA, October 16- 18, 2012, 2012, pp. 241–252.
A. Gorla, I. Tavecchia, F. Gross, and A. Zeller, “Checking app behavior against app descriptions,” in ICSE’14: Proceedings of the 36th International Conference on Software Engineering, 2014.

- Too often the authors make some unsubstantiated claims. I would have tried to support such claims by a minimum of qualitative analysis, e.g.:
 p3: "Such a problem not only indicates quality issues, but could also indicate that these apps may contain functionality which is probing for open or available permissions"
 p3: "Under permissions may cause an app to crash when it attempts to utilize the permission it has not been granted. While the user will be very unlikely to know that the crash was due to an under-permission, the encountered defect could still impact their review of the app."

- I was not surprised by the lack of correlation with under-permission. Why this should create problems? Was it really an under-permission that affected the app behavior? Again, I understand this is a short paper, but all is left to our possible interpretation

- Stowaway is obsolete as permission analysis tool. I suggest you use PScout instead:
Kathy Wain Yee Au, Yi Fan Zhou, Zhen Huang and David Lie.  PScout: Analyzing the Android Permission Specification . In the Proceedings of the 19th ACM Conference on Computer and Communications Security (CCS 2012). October 2012.

A final suggestion but this is more for future work than for this paper. There are many papers correlating user rating with a number of variables, and different papers came out with different findings (e.g. reference [20] for API defects, but also previous work by one of the author, not mentioned here, relating ratings with ads and with various other characteristics of the app). Above all the work:
Yuan Tian, Meiyappan Nagappan, David Lo, Ahmed E. Hassan:
What are the characteristics of high-rated apps? A case study on free Android Applications. ICSME 2015: 301-310

Now it would be interesting to see how such permission metrics weight in a "global" model relating many app characteristics to the rating...


----------------------- REVIEW 4 ---------------------
PAPER: 27
TITLE: Examining the Relationship between Security Metrics and User Ratings of Mobile Apps: A Case Study
AUTHORS: Daniel Krutz, Casey Klimkowsky, Shannon Trudeau, Adam Blaine, Meiyappan Nagappan, Andrew Meneely and Sam Malachowsky


----------- REVIEW -----------
The paper presents an empirical study on 4000 Android apps to
highlight correlations between security metrics and user ratings on
the Google Play store. The authors used static analysis tools to see
which apps are overprivileged (i.e. apps that require more
permissions than the ones they actually use), but found only weak
correlations. The only strong correlations that they found is related
to low user ratings and number of permissions that the apps requests.

--

I think this study tackles a very important aspect of mobile
apps. Most users, in fact, tend to complain if an app does not work,
if some key features are missing, or if the prices are too high, and
they might give less importance to security aspects. There exist
already studies on how users perceive requested permissions, and some
of these studies show that users do not like apps that require lots of
permissions, since they believe it is potentially risky for their
privacy. In this respect, I believe that this study does not add any
new insight to this topic, but rather confirms what was known already
by showing that apps with lots of permissions have few downloads (the
authors stated it was challenging to identify apps with more than 1K
downloads below 3 stars rating), and tend to have low rating
scores. The message that I get from this study is that users do not
like apps that might threaten their privacy, but as I said, this was
already known [19].

Beside the lack of novelty of the study, I have three major comments:

- I do not buy the argument about "underprivileged" apps being
  necessarily bad. It is correct to state that apps that try to use
  functionalities without having asked for permissions will lead to
  runtime exceptions, but the authors completely ignore the fact that
  apps may have invocations to the Android API in *dead code*, as it
  is often the case. This is very common since apps use external
  libraries that might offer many more functionalities than the one
  the app uses. As a consequence, to assess what the authors say the
  static analysis should report on those API calls that for sure
  happen without having the corresponding permission. Another aspect
  to keep into account is that the permission mapping used by the
  analysis tools is *not complete*. This is, in essence, to say that
  this study cannot say anything regarding underprivileged apps,
  unless a careful manual inspection were done (but this is not the
  case).

- I do not understand why "to create a fair comparison" the authors
  decided to normalize the security metrics by LOCs. In general I
  believe that the size of an app does not have an influence on how
  privacy invasive this is. Moreover, as I already said, apps
  typically include a large amount of code that they do not use
  (because of third party libraries) and I am afraid that this aspect
  was not considered here.

- The authors state that "low-rated apps suffer from security
  vulnerabilities". I am not sure what the definition of security
  vulnerability for the authors is, but my definition is that this is
  a weakness in the code that allows attackers to do things they
  should not be allowed to do. The only thing that this study shows is
  that apps that request lots of permissions are also
  low-rated, and apps that request lots of permissions do not
  necessarily suffer from security vulnerabilities. In fact, the vast
  majority of these apps are a security threats themselves (i.e. they
  are spyware, mainly because they include quite invasive third-party
  ad libraries). These two scenarios are very different, and the
  authors seem to confuse them.

- Last but not least, Section 3.1 contains a clear contradiction for
  me. First the author say "The only reasonable strung correlation we
  found was within the number of permissions that were requested by
  low-rated apps." but few paragraphs later they say "it comes to no
  large surprise that we found only a weak correlation between the
  number of requested permissions and user ratings." The authors
  should either stress on the only correlation that they found, or
  should simply say that the whole study did not lead to anything
  conclusive. They should also clarify that the only string
  correlation comes from a single category of apps, while for other
  categories they could not find anything relevant.

Thus, even if I find this research line interesting, I believe that
the only finding that the author present was already known, and I
therefore do not see reasons why this paper should be accepted for
publication.


-------------------------  METAREVIEW  ------------------------
PAPER: 27
TITLE: Examining the Relationship between Security Metrics and User Ratings of Mobile Apps: A Case Study

This paper investigates whether security metrics, extracted by Static Analysis Tools, correlate with user ratings of Android apps.

Main Strengths: 
-	The reviewers agree that this is a timely and important topic. 
-	The paper is well written. 

Main Weaknesses: 
-	The insights added do not add much to what was already known (R4) and also that some related but highly relevant work was not discussed (R3). 
-	There were a number of shortcomings with the methodology as noted by all reviewers.  In particular, other variables that could explain the correlations were not investigated (including variables that are more observable by the users which could influence the user rating).  Please refer to the individual reviews for other weaknesses noted.
-	The paper contains a number of claims that warrant more investigation using manual methods. 

In summary, the reviewers felt the paper is not ready at this time for publication at MSR but the paper points to some interesting avenues for future research. 
