Dear Daniel, 

Thanks for your submission to the 12th Working Conference on Mining Software Repositories (MSR 2015). We regret to inform you that your paper, 

Examining the Relationship between Security Ratings and User Ratings of Mobile Apps: A Case Study 

has not been accepted for inclusion in the conference program. 

We received a record number of 136 submissions (106 full and 30 short research and practice paper submissions); out of the 106 full paper submissions, we accepted 32 papers, and out of the 30 short paper submissions, we accepted 10 papers. These are acceptance rates of 30% for full papers and 33% for short papers. 

Every paper received at least three reviews by members of the PC, and was carefully discussed by PC members until a consensus was reached. For the first time MSR applied the leading reviewer model. For each paper a leading reviewer was assigned to lead the discussion and summarize the results in a meta review sent to the authors. We personally examined all of the reviews, meta reviews, and online discussion when making the decisions. All decisions were only based on the quality of the papers and on the outcome of the discussions. We did not target any minimum nor maximum number of papers to be accepted. 

MSR targets solid research contributions. There are many promising research avenues described in rejected papers, and in many cases the PC just felt the ideas needed further development before they would be accepted. 

We enclose below the reviewers' comments to your submission and the meta review, which we hope you will find useful for your future research. 

We hope that you will be able to attend the MSR conference in Florence, Italy, on May 16th - 17th, 2015. 

Sincerely, 

Romain Robbes and Martin Pinzger 
MSR 2015 Program Co-chairs.


----------------------- REVIEW 1 ---------------------
PAPER: 120
TITLE: Examining the Relationship between Security Ratings and User Ratings of Mobile Apps: A Case Study
AUTHORS: Daniel Krutz, Meiyappan Nagappan and Andrew Meneely


----------- REVIEW -----------
This paper examines the relationship between security metrics and user ratings of mobile apps. It collects 798 Android apps with user ratings less than 3 and 861 apps with user ratings of 3 or greater. It leverages two static analysis tools, Stowaway and Androrisk, to analyze security flaws in these applications, and compare the two distinct groups of applications to evaluate whether low- and high-rated apps have different distributions for each of the security related metrics. The evaluation shows that high-rated apps typically have higher security risks. The paper recommends developers to leverage the static analysis tools to check and fix security flaws before releasing their applications.

I feel a little surprised when the paper concludes that high-rated apps typically have higher security risks. Two security analysis tools are used: Stowaway and Androrisk. Stowaway reports that high-rated apps have a larger incidence of both over-permissions and under-permissions, indicating that high-rated apps have more security risks. However, Androrisk assigns a higher FuzzyRisk score to low-rated apps than high-rated apps, meaning that low-rated apps have more security risks. I don’t understand why authors simply ignored the FuzzyRisk scores and concluded that low-rated apps contain more security flaws than high-rated ones.

Even if high-rated apps have higher security risks than low-rated apps, I don’t believe that the comparison between the two groups is fair. The apps are different in terms of functionality, complexity, size, etc. Even if low-rated apps have fewer security risks, we can still not recommend users to download low-rated apps instead for security reasons. When we compare an OS and a toy program, even if the former one contains a lot of bugs, no one would like to replace it with the latter bug-free toy program, because the latter one cannot provide the functionality of OS. If the authors want to examine whether security trade-offs with other properties such as usability and performance, they should at least identify some apps with similar functionalities. In this way, they can screen out as many irrelevant factors as possible in order to observe the effect of security mechanism implementations to program usability and performance.


----------------------- REVIEW 2 ---------------------
PAPER: 120
TITLE: Examining the Relationship between Security Ratings and User Ratings of Mobile Apps: A Case Study
AUTHORS: Daniel Krutz, Meiyappan Nagappan and Andrew Meneely


----------- REVIEW -----------
In this paper the authors are concerned with the connection between metrics relating to security, garnered through the analysis of permissions requested by an, and the ratings ascribed to be out by its users. 

This is important, because we already know from previous research that user ratings are highly correlated with the number of downloads in apps stores [ref 7], and therefore that reviews are directly correlated with the revenue that accrues on the deployment of the act. 

Therefore, if there is an established relationship between security metrics and user ratings, then the security metrics could have a knock-on effect on the revenue achievable for a particular app. On this basis, the results of this paper have a high potential for actionability.

The paper is a short paper, and the authors do a good job of delivering research findings, novel ideas, and interesting conclusions and actionable results in this relatively limited space.

The results are based on analysis of 798 apps with ratings lower than 3  stars, and 861 apps with equal or greater than 3 stars. Based on this partition of the data, the authors find significant differences in security metrics pertinent to the lower rated and higher rated apps. These are the primary findings of the paper, and I believe it is worthy of publication, based on this actionability


minor points for corrections and revise (and maybe future work):-

1. Perhaps the authors could spend some time on proofreading for the camera ready copy. For example, the objective of the study is set out in italicised paragraph in the introduction, which is a great benefit. However, the first sentence of this italicised paragraph contains a very obvious typo ("the the"),  and the fact that this escaped the authors quality-control suggests a rush to submission :-).  Nevertheless, the paper is strong, and these typo issues are minor details that can be easily fixed.

2. The authors use an inferential statistical test to distinguish two different categories of apps, based on  lower and higher ratings achieved by the sets of apps. This is a very crude and approximate way of determining whether there is a correlation between rating and security metrics.  It can only get a yes or no determination, and also it affords the researchers the opportunity to *choose* the split point to suit their research question, potentially introducing a threat to validity.  

That is, in almost any data, I can find a split point such that I have partitioned the data into two subsets that will show statistically significant differences between them. I'm not saying that's what the authors did, since the split point is a very natural one, and I'm sure they didn't search for one that shows the result they seek to find. Nevertheless, this issn  observation that indicates the fragility of the research methodology.

I would like to recommend that the authors develop this work for a fuller journal version of the paper, in which they use correlation analysis. Given that their narrative in the paper talks about the *correlation* between security metrics and user ratings, it's surprising that they didn't use any *correlation* analysis. 

A simple Spearman or Kendall rank correlation test would provide a much stronger sense of the degree of correlation between security metrics and user ratings. Furthermore, if some transformation of the data could produce a Pearson correlation, then this would suggest that the predictive model could be built from the results reported in the paper. 

Such a predictive model would be hugely valuable as an outcome of this research agenda: with it developers could actually predict the rating based on their security preferences. This would have a strong motivating effect the developers, who would be disinclined to request permissions they didn't actually require, for example




OVERALL

I'm confident that this paper should be accepted for the conference, and I hope that the minor revision requests are helpful to the authors.


----------------------- REVIEW 3 ---------------------
PAPER: 120
TITLE: Examining the Relationship between Security Ratings and User Ratings of Mobile Apps: A Case Study
AUTHORS: Daniel Krutz, Meiyappan Nagappan and Andrew Meneely


----------- REVIEW -----------
SUMMARY:

This paper presents an empirical study examining a possible relationship between security and user ratings for android applications. The authors design the study as the following: (1) Firstly, the collect apps from GooglePlay and cluster the apps into two groups – apps with user rating having 3 stars and higher and apps having ratings with lower than 3 stars; (2) Secondly, they removed the apps with less than 1K downloads; (3) Next, they ran two open source static analysis tools, Stowaway and Androrisk, on the apps and collected the analysis results; (4) Finally, they analyze the data collected from the tools along with user ratings data. The major conclusions are that several security metrics were higher (more risky) in higher rated apps as compared to lower rated apps. The final conclusion is that the user ratings may be a more all-encompassing measure that does not necessarily depend on security only. 

COMMENTS:

Overall, this is a well-written paper that poses an interesting question. However, the major problem with the paper is that the analysis conducted is purely quantitative in nature and does not offer any insights or explanations into WHY security related metrics may correlate with user ratings at all. In fact, many of the security related metrics are not directly perceived by the users of the apps, so it is not clear why and how these metrics should be related to the ratings at all (for example, one obvious question is whether users mention security related concerns in their reviews at all).

Also, the results in the paper do not support the research question. While the results are potentially interesting (“Higher-rated apps typically have higher security risks”), there are no tangible explanations on why this may be the case. At this point I am not convinced that this is not just accidental correlation (for example, these security metrics are likely to have higher values for larger apps).

The study relies on Androrisk to examine the usage of security permissions to support the main argument. However, permission usage does not necessarily imply permission leaking. The granularity checking is not enough in this study. It is also important to check how permissions have been actually used. In addition, an important metric does not support authors’ point, such as FuzzyRisk, which is an overall risk score for an app.   

While discussing the results in the Table 2, the authors mention that “these results imply that even though lower-rated apps request more permissions they actually use all of them. Whereas, higher-rated apps are asking for permissions that they do not even use, and this is quite dangerous.”   I did not see why lower rated apps are safer based on this argument. If a potential usage is “dangerous” in higher rated apps, why the real usages are NOT more dangerous in lower rated apps? Especially, if lower-rated apps request more permissions. 

Many metrics are likely to be correlated with app size and categories of the apps.  For example, larger apps may require more permissions in general and, thus, the chances are higher that those apps will underutilize some of those permissions. I think this point needs to be thought through more carefully. 

Minor comments:

The title of Table 1 needs to be updated. Clearly, it is not only about Filesize setting. In addition, it would benefit from adding more details in the caption.  

False positives resulting from running these tools have not been examined and, thus, are currently considered in the correlation analysis. This is a major threat to validity that deserves some discussion.


-------------------------  METAREVIEW  ------------------------
PAPER: 120
TITLE: Examining the Relationship between Security Ratings and User Ratings of Mobile Apps: A Case Study

This paper presents an empirical study examining a possible relationship between security and user ratings for android applications. Overall, this is a well-written paper that poses an interesting question. The reviewers would like to encourage the authors to address the comments in the reviews and resubmit this work in the future.

Strengths:

1. An interesting and novel problem.
2. Well-written paper.

Weaknesses:

1. The analysis conducted is purely quantitative in nature and does not offer any insights or explanations into why security related metrics may correlate with user ratings.
2. Also, the results in the paper do not directly support the research question.
3. Potentially confounding factors need to be analyzed (e.g., app size).
4. False positives resulting from running the tools have not been examined and, thus, are currently considered in the correlation analysis.
5. FuzzyRisk scores are currently ignored.