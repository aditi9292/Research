Dear Dr. Daniel Krutz:

We are sorry to inform you that the following submission was not selected by the program committee to appear at SAC 2016: 

      Evaluating Effectiveness of Static Analysis Testing
           Tools On Android Applications

The selection process was very competitive. Due to time and space limitations, we could only choose a small number of the submitted papers to appear on the program. 

We have enclosed the reviewer comments for your perusal.

If you have any additional questions, please feel free to get in touch.

Best Regards,
Mercedes Merayo and Gwen Salaün
SAC 2016, SVT - Software Verification and Testing

============================================================================
SAC 2016 Reviews for Submission #1032
============================================================================ 

Title: Evaluating Effectiveness of Static Analysis Testing Tools On Android Applications

Authors: Daniel Krutz, Matthew Thornton, David Kubarycz, Amol Pantvaidya and Abhishek Dale ============================================================================
                            REVIEWER #1
============================================================================ 


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------

          Technical Content and Accuracy: 2
                Significance of the Work: 3 Appropriate Title, Introduction, and Conclusion: 6
                    Overall Organization: 6
                 Appropriateness for SAC: 6
          Style and Clarity of the Paper: 4
                  Originality of Content: 4
                  OVERALL RECOMMENDATION: 2


---------------------------------------------------------------------------
Comments
---------------------------------------------------------------------------

SUMMARY
This paper compares 3 popular tools for extended static checking on Android application wrt their defect finding capabilities. 

EVALUATION
Unfortunately, I do not support acceptance of this paper. 

The topic is relevant, the paper is reasonably well-written. However, the paper suffers from a number of drawbacks. 

* The main problem is that the sample set (i.e., the number of apps tested) is far too small: just 3 out of the millions of apps that are available. 

* It is not so clear how the experiments were done exactly. The information in Sect 3 does not suffice here: were any bugs seeded? 
- Focussing on common patterns may give biased results

* There are many items that deserve a more elaborate explanation. 
- what is so specific about Android apps when compared to regular programs? How does that effect static checking?
- why these tools? why are other tools disregarded?  
- what are the workings of the extended checkers? 
- the tools are compared wrt precision, recall and F-measure. Please elaborate on their definitions and explain why these criteria are used. 

I would encourage the authors to dramatically extend their experimental validation.

============================================================================
                            REVIEWER #2
============================================================================ 


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------

          Technical Content and Accuracy: 5
                Significance of the Work: 6 Appropriate Title, Introduction, and Conclusion: 7
                    Overall Organization: 7
                 Appropriateness for SAC: 7
          Style and Clarity of the Paper: 6
                  Originality of Content: 4
                  OVERALL RECOMMENDATION: 4


---------------------------------------------------------------------------
Comments
---------------------------------------------------------------------------

The paper discusses the effectiveness of three static analysis testing techniques in the context of android devices. The paper is well written and structured. 

Concerns:
- How was the defect selection done? Are the defects the rulesets? What are the high-priority (major? critical? blocker?). The Step 1 in Section 3.2 requires more information, as it is very important to understand the results.

- Wouldn't the study be more comprehensive if the authors considered all the rulesets? Is this unfeasible? Do the authors think the results would be essentially the same when considering all the rules?

- The authors need to quote numbers about the used programs for the readers to understand the complexity of the empirical evaluation.

- I do not follow the following paragraph: 'Each of the three examined tools have different settings which may be altered depending on the desired precision and recall for each tool. In each case, we altered the default settings to create the best precision and recall for each tool. These tool settings are available on our project website.' Are you referring to the subject tools or the static analysis tools? I guess I am confused with the word 'examined'.

- How are the defects injected in the applications? All at the same time? One at the time?

- Android Lint with .833 --> PMD with .833. Just after that paragraph I was expecting a summarizing paragraph about the results. 

- These are empirical results and there are several threats to the validity that need to be discussed before section 5. 

- Related work could discuss approaches that combine static analysis techniques (Lint, in fact) with dynamic analysis:

Machado, Pedro, et al. "MZoltar: automatic debugging of Android applications."
Proceedings of the 2013 International Workshop on Software Development Lifecycle for Mobile. ACM, 2013.

============================================================================
                            REVIEWER #3
============================================================================ 


---------------------------------------------------------------------------
Reviewer's Scores
---------------------------------------------------------------------------

          Technical Content and Accuracy: 2
                Significance of the Work: 4 Appropriate Title, Introduction, and Conclusion: 3
                    Overall Organization: 2
                 Appropriateness for SAC: 4
          Style and Clarity of the Paper: 3
                  Originality of Content: 4
                  OVERALL RECOMMENDATION: 3


---------------------------------------------------------------------------
Comments
---------------------------------------------------------------------------

This paper reports on a study that has been done to evaluate the effectiveness of 3 static analysis testing tools for Android: FindBugs, PMD and Android Lint. 
Effectiveness is defined by the amount of defects found in amongst different quality characteristics  ‘compatibility’, ‘concurrency’, ‘correctness’, ‘maintainability’, ‘performance’, ‘reliability’, ‘security’, and ‘usability.’

Each tool contains a set of defect patterns that are used during code inspections to identify defects and each of these defect patterns are unique to the tool itself and cannot be directly compared to other tools.  This is why in the paper a repository is created that contains a common set of 50 potential defects for the 3 tools.

This repository (called defect oracle in the paper) is the most important item of this paper and its exact construction, the decisions taken and its contents are vital for the validity of the results presented. Unfortunately, the description is not sufficient to review the results.  A clear threats to validity section is missing. Moreover, this paper seems to be written up in a hurry. Some English sentences do not read well and some paragraphs are repeated (for exaple the short descriptions of the static analysis tools appears in 3.1 and 3.3).

Minor things:
- I am not sure the terminology ``defect oracle'' is right and causes confusion to this reviewer ;-)
- Why the number 50 defect patterns in the oracle?
- It would be nice to include some more information and metrics aout the SUTs you have chosen in section 3.3
- Why does Table 2 show nothing about the priorities?
