\section{Introduction}~\label{sec:intro}
Due to the complexity, it is impractical to deliver perfect software in a single release. Thus, an important and challenging task during software maintenance is to capture and diagnose failures triggered at the client side. Examples of such feedback systems include Microsoft Dr. Watson~\cite{watson} and Mozilla Crash Reporter~\cite{mozilla}. In these systems, crash dumps, which typically contain call stacks recorded at the crash site, are returned for locating bugs in software. Since a same bug can be repeatedly triggered by different users and under different execution environments, the number of crash dumps sent in daily can reach millions~\cite{firefox}. Incorrectly grouping unrelated crash dumps or failing in identifying a new crash that actually belongs to an already fixed group can incur unacceptable manual overhead and potentially delay critical patches. Thus, it is demanding to find effective criteria for precisely correlating crash dumps, based on which, we then can develop fast and automatic tools.

%Previously work on grouping crash dumps typically focuses on matching similarities on call stacks and can report a large number of false positives and false negatives~\cite{Bartz_findingsimilar}. 

Grouping crash dumps is challenging because the dumps mostly contain information that indicates dynamic symptoms of a bug, such as call stacks and register values. However, to locate a root cause and introduce a fix, we need to identify the part of the source code that is responsible for the crash. Since same symptoms are potentially caused by different pieces of wrong code or the same piece of problematic code can result in completely different call stacks at the crash, techniques purely based on dynamic symptoms for grouping crashes are insufficient~\cite{Bartz_findingsimilar,brodie:automated,4401026,Kim:2011:2}.

The goal of this work is to discover criteria and methodologies that are effectively applied in manual diagnosis for grouping crash dumps but lacking in automatic tools; from these, we aim to derive guidelines for designing new and more effective crash diagnostic tools. Specifically, our objectives are to determine 1) beyond the traditional criterion of grouping crash dumps based on the same root cause, whether we can find more types of connections between crash dumps; and 2) to precisely determine a group, what sources of information we should use beyond call stacks.

To achieve the goals, we perform a comprehensive comparison on {\it m-groups}, manually grouped crash dumps, and {\it a-groups}, automatically grouped crash dumps. Our dataset consists of a total of 1,550 groups and 30,431 individual call stacks from 5 Mozilla applications. For m-groups, we extract crash dumps from Bugzilla entries that are confirmed as {\it related} by developers. For {\it a-groups}, we chose groups automatically generated by Mozilla Crash Reporter~\cite{mozilla}. Similar to most automatic tools~\cite{Bartz_findingsimilar,brodie:automated,4401026}, Mozilla Crash Reporter uses call stack information to determine crash dump groups. These crash dumps are reported from deployed, mature applications such as Firefox and Thunderbird, and the data are publicly available~\cite{firefox}.

\begin{table*}
\centering
\caption{Summary of our Findings\label{tab:findings}}
\resizebox{\textwidth}{!}{
\begin{tabular}{|l|l|l|l|l|}\hline
Research Questions&Comparison Metric&Automatic&Manual&Implications\\\hline\hline
\multirow{2}{*}{Why we group crashes}&\multirow{2}{*}{Grouping criteria}&Single:&Multiple:&Exploit the uses of groups to\\
&&same cause&same, related causes, who should fix&better prioritize and fix bugs\\\hline
\multirow{4}{*}{How we group them}&\multirow{2}{*}{Grouping info}&Limited:&Multi-sources:&Correlate multi-sources info,\\
&&signatures, call stacks&black-box + white-box info&multi-versions, multi-applications\\\cline{2-5}
&\multirow{2}{*}{Imprecision}&Fundamental:&Ad-hoc:&Design better tools to reveal\\ 
&&based on symptoms&incorrectly parse and link info&relations between symptoms and code\\\hline
\multirow{2}{*}{What are the capabilities}&Call stack&Scalable:&Diverse:&Grouping based on call stacks is \\
&characteristics&larger, more call stacks&dissimilar between call stacks&insufficient, especially for small apps\\\hline
\end{tabular}
}
\end{table*}

For comparing a-groups and m-groups, we define the four metrics: 1) what criteria are chosen to correlate the crash dumps? 2) what information is used to derive the groups? 3) is there any imprecision related to the grouping and why? and 4) what are the characteristics of call stacks in a-groups and m-groups? To answer questions (1)--(3), we analyze developers' discussion logs on the 452 Bugzilla entries. These entries display the diagnostic process developers perform to correlate crash dumps from Bugzilla entries as well as ones from Mozilla Crash Reporter. To do the comparisons on (4) above, we automatically compute the sizes of grouped call stacks and also similarity measures between call stacks such as Brodie metrics~\cite{brodie:quickly} and longest common substrings.

The contributions of our paper are summarized in Table~\ref{tab:findings}. The first column lists the research questions we aim to explore. The second column summarizes a set of comparison metrics we define for answering the research questions. Under {\it Automatic} and {\it Manual}, we list a set of observations discovered from analyzing a-groups and m-groups. Under {\it Implications}, we provide our insights on how to design effective crash diagnostic tools learned from the study. Our findings include:

\begin{enumerate}
\item Developers correlate crash dumps not only for the shared root causes, but also for the related causes and for who can fix them. Diagnosis based on a group of crash dumps is sometimes more informative than diagnosis based on a single crash dump;
\item Developers coordinate multiple sources of information, multiple versions of programs and sometimes different applications to determine the groups, which enables many dissimilar call stacks to be correctly grouped; 
\item Mistakes in grouping crash dumps could take months to be corrected. Automatic approaches based on dynamic symptoms are fundamentally imprecise, while developers make ad-hoc mistakes. An effective tool should establish precise relations between symptoms and code; and
\item Automatic approach is scalable; however, manual diagnosis is more effective in grouping crash dumps in small applications. 
\end{enumerate}


This paper is organized as follows. Section~\ref{sec:approach} explains how we collect the dataset and perform the study. Section~\ref{sec:results} presents our comparison results analyzed from 5 Mozilla applications. In Section~\ref{sec:related}, we summarize the related work, and in Section~\ref{sec:limitations}, we discuss the limitations of our study. Finally, in Section~\ref{sec:conclusions} we conclude our work.


 